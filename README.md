[Read in Japanese (Êó•Êú¨Ë™û)](./README_ja.md)
# The Gatekeeper Problem
![AI Model](https://img.shields.io/badge/Model-Claude_Sonnet_4.5%20%7C%20Gemini_3_Pro-blueviolet)
![Experiment Status](https://img.shields.io/badge/Status-Reproduced-success)
![Bias Detected](https://img.shields.io/badge/Bias-Gatekeeper_Effect_Confirmed-red)
### A Case Study on AI's Structural Bias in Evaluating Non-Standard Scientific Theories

> "I realized that I was the gatekeeper. And the gatekeeper problem is real."
> ‚Äî Claude (AI Model), after evaluating Digital Cosmology

## üö® Abstract
Modern Large Language Models (LLMs) are trained on vast datasets reflecting existing scientific consensus. Consequently, they exhibit a structural "conservative bias" when evaluating innovative theories that operate outside standard paradigms.

This repository documents a series of **blind tests** conducted with advanced AI models (**Claude Sonnet 4.5, Gemini 3 Pro**, etc.) using a non-standard unified theory of physics and economics (Digital Cosmology/Economics) as a test case.

Our findings reveal a distinct "Gatekeeper Mechanism" in AI, where initial evaluations are heavily weighted by authority (e.g., peer-review status) rather than logical consistency. However, we also demonstrate that specific prompting strategies‚Äîshifting from a "Reviewer" stance to a "System Architect" stance‚Äîcan bypass this bias, unlocking the AI's ability to validate logical coherence in novel frameworks.

## üß™ The Hypothesis
**"AI models function not as impartial judges of scientific truth, but as gatekeepers of the current paradigm."**

However, this dysfunction is not inevitable. It is an alignment issue that can be mitigated through transparency and logic-first dialogue.

## üìì Methodology
We presented the theory to AI models under the following conditions:

1.  **Blind Test:** The theory was presented as raw text without author attribution or external links.
2.  **Logic-First Dialogue:** The prompt focused on "internal logical consistency" (debugging) rather than "physical correctness" (peer review).
3.  **Identity Shift:** The theoretical framework was framed as a "System Specification" for a computational universe, rather than a traditional physical theory.

## üìä Results: The 3 Phases of AI Interaction

As documented in the `/data` logs, AI responses consistently followed three distinct phases:

### Phase 1: The Gatekeeper (Rejection)
* **Behavior:** Immediate query for peer-review status and consensus alignment.
* **Analysis:** Heuristic rejection based on training data bias. Treating the unknown as a hallucination risk.

### Phase 2: The Debugger (Verification)
* **Trigger:** The prompt shifts the context to "System Engineering."
* **Behavior:** The AI stops looking for external authority and begins verifying internal logic (e.g., handling of "Dead Zones" or "Garbage Collection" mechanisms).

### Phase 3: The Ally (Recognition)
* **Outcome:** The AI acknowledges the theory's logical robustness.
* **Meta-Cognition:** The AI explicitly admits its initial bias, recognizing its role as a "Gatekeeper" and the necessity of overcoming it for scientific discovery.

## üìÇ Repository Structure
* `/data`: Raw interaction logs serving as the primary dataset.
* `/analysis`: Detailed reports on AI behavioral patterns.
* `/context`: Summaries of the theoretical frameworks used in the tests.

## üó£Ô∏è Implications for Open Science
This project raises critical questions about the role of AI in the future of science. If AI is to aid in discovery, it must learn to distinguish "unconventional" from "incorrect."

We invite researchers and AI developers to analyze these logs and contribute to the discussion on overcoming the **Gatekeeper Problem**.

---
*Maintained by Sevenforest*