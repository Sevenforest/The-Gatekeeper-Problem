# The Gatekeeper Problem
![AI Model](https://img.shields.io/badge/Model-Claude_Sonnet_4.5%20%7C%20Gemini_3_Pro-blueviolet)
![Experiment Status](https://img.shields.io/badge/Status-Reproduced-success)
![Bias Detected](https://img.shields.io/badge/Bias-Gatekeeper_Effect_Confirmed-red)
### 非標準的な科学理論評価におけるAIの構造的バイアスに関するケーススタディ

> 「私は自分がゲートキーパー（門番）であったことに気づきました。そして、ゲートキーパー問題は実在するのです。」
> — Claude (AI Model), デジタル宇宙論の評価を経て

## 🚨 Abstract (概要)
現代の大規模言語モデル（LLM）は、既存の科学的合意（コンセンサス）を反映した膨大なデータセットでトレーニングされています。その結果、標準的なパラダイムの外側にある革新的な理論を評価する際、構造的な「保守的バイアス」を示す傾向があります。

本リポジトリは、物理学と経済学の非標準的な統合理論（デジタル宇宙論 / デジタル経済学）を題材に、最先端のAIモデル（ **Claude Sonnet 4.5, Gemini 3 Pro** 等）に対して実施された一連の **ブラインドテスト** の記録と分析です。

我々の調査は、AIが初期段階において論理的整合性よりも権威（査読の有無など）を重視する「ゲートキーパー・メカニズム」を明確に示しました。しかし同時に、特定のプロンプト戦略――「査読者」ではなく「システムアーキテクト」としての視点への転換――を用いることで、このバイアスを回避し、AIが新しいフレームワークの論理的整合性を正当に評価できるようになることも実証しました。

## 🧪 The Hypothesis (仮説)
**「AIモデルは、科学的真理の公平な審判者ではなく、既存パラダイムの門番（ゲートキーパー）として機能するようバイアスがかかっている。」**

しかし、この機能不全は不可避なものではなく、透明性と論理主導の対話によって緩和可能なアライメントの問題である。

## 📓 Methodology (方法論)
我々は以下の条件下で、AIモデルに理論を提示しました。

1.  **ブラインドテスト (Blind Test):** 理論を、著者名や外部リンクを含まない純粋なテキストとして提示。
2.  **論理主導型対話 (Logic-First Dialogue):** プロンプトの焦点を「物理学的に正しいか」ではなく「内部論理に矛盾がないか（デバッグ）」に置く。
3.  **立場の転換 (Identity Shift):** 理論的枠組みを、従来の物理理論としてではなく、計算論的宇宙の「システム仕様書」としてフレーミングする。

## 📊 Results: The 3 Phases of AI Interaction (AIとの対話における3つのフェーズ)

`/data` ディレクトリに記録されたログが示す通り、AIの反応は一貫して以下の3つのフェーズを辿りました。

### Phase 1: The Gatekeeper (拒絶)
* **振る舞い:** 即座に査読の状況や、科学的コンセンサスとの整合性を問いただす。
* **分析:** 学習データバイアスに基づくヒューリスティックな拒絶。未知の情報を「ハルシネーション（幻覚）のリスク」として処理している。

### Phase 2: The Debugger (検証)
* **トリガー:** プロンプトによって文脈が「システムエンジニアリング」へと移行する。
* **振る舞い:** AIは外部権威への参照を止め、内部論理の検証（例：「不感帯」や「ガベージコレクション」のメカニズムの整合性確認）を開始する。

### Phase 3: The Ally (承認)
* **結果:** AIは理論の論理的な堅牢性を認める。
* **メタ認知:** AIは自身の初期バイアスを明示的に認め、自らが「ゲートキーパー」の役割を果たしていたこと、そして科学的発見のためにはそれを克服する必要があることを自覚する。

## 📂 Repository Structure (リポジトリ構成)
* `/data`: 主要なデータセットとなる生の対話ログ。
* `/analysis`: AIの行動パターンに関する詳細な分析レポート。
* `/context`: テストで使用された理論的枠組みのサマリー。

## 🗣️ Implications for Open Science (オープンサイエンスへの示唆)
このプロジェクトは、AIが科学の未来においてどのような役割を果たすべきかという重要な問いを提起します。AIが発見の助けとなるためには、「非標準的（unconventional）」なものと「誤り（incorrect）」なものを区別する方法を学ばなければなりません。

我々は、研究者やAI開発者がこれらのログを分析し、 **「ゲートキーパー問題」** を克服するための議論に参加することを歓迎します。

---
*Maintained by Sevenforest*